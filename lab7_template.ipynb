{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581f5f99",
   "metadata": {},
   "source": [
    "# Lab 7 Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682e0d0",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a3fe1",
   "metadata": {},
   "source": [
    "### Packages to install\n",
    "\n",
    "- ucimlrepo (for grabbing datasets)\n",
    "- transformers\n",
    "- autogen\n",
    "- jupyter (if you aren't on the latest version, there's a dependency in tqdm that complains)\n",
    "\n",
    "#### Install with pip\n",
    "\n",
    "- pip install ucimlrepo transformers autogen jupyter\n",
    "\n",
    "### From lab 3\n",
    "\n",
    "- pandas, numpy, matplotlib.pyplot, seaborn, tqdm, torch, sklearn\n",
    "\n",
    "#### Install everything with pip\n",
    "\n",
    "- pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eac90980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install stuff on colab\n",
    "#!pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54903e1",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "The goal of this lab is to give you an idea of how you could use agents to help with physics tasks. It will also introduce you to AutoGen, one of the more popular frameworks at the moment for designing custom agentic workflows. We won't make use of all the tools it provides, just the very basics. Note also that many of the most interesting things one can do with AI-powered agents (see topics like retreival augmented generation (RAG)) require very large models to be performant, and many techniques require continuous/repeated training. This means large resource requirements, so for this lab we will just be using a very small LLM (Llama: TinyLlama-1.1B-Chat-v1.0). The results from this are nowhere near as good as something like chatGPT, but it should give you an idea of how a more advanced model (or models) might be able to do something really helpful/cool. This is an area of current research, so it will be interesting to see what they can do!\n",
    "\n",
    "Also because of the limited size of the model, the text parsing needs to be very mechanical, and in some places a bit obtuse. The better (and ideally specially trained for an agentic workflow, see RAG) your model is, the more this can be relaxed. If you're interested in working with agents in a more user-friendly way (hiding a lot of the mechanics that are on display here), check out sites like n8n or Google Gemini (be aware that these can require you to provide a lot of permissions). You can also feel free to replace TinyLlama with a call to a larger model using API keys if you have some.\n",
    "\n",
    "Finally, we would like to emphasize the use of copilot/similar tools for this lab in particular. These are agents too! And they are definitely the most performant agents you have easy access to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f282e",
   "metadata": {},
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bad3ed",
   "metadata": {},
   "source": [
    "- There are two datasets, \"mnist\" (from Lab 3) and \"solar_flare\"\n",
    "- Try to get everything working with mnist, then try adding solar_flare\n",
    "    - This is one way agents can be helpful, since they can analyze a dataset you've never seen before and take a first crack at it much faster than you can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391c968",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "835b6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from autogen import ConversableAgent, AssistantAgent\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85da87",
   "metadata": {},
   "source": [
    "### Lab 3 code (run and then minimize this)\n",
    "\n",
    "- Mostly just lab 3 code packaged into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f462f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset in Numpy\n",
    "def load_mnist_data():\n",
    "    # 1000 training samples where each sample feature is a greyscale image with shape (28, 28)\n",
    "    # 1000 training targets where each target is an integer indicating the true digit\n",
    "    mnist_train_features = np.load('mnist_train_features.npy') \n",
    "    mnist_train_targets = np.load('mnist_train_targets.npy')\n",
    "\n",
    "    # 100 testing samples + targets\n",
    "    mnist_test_features = np.load('mnist_test_features.npy')\n",
    "    mnist_test_targets = np.load('mnist_test_targets.npy')\n",
    "\n",
    "    # Print the dimensions of training sample features/targets\n",
    "    #print(mnist_train_features.shape, mnist_train_targets.shape)\n",
    "    # Print the dimensions of testing sample features/targets\n",
    "    #print(mnist_test_features.shape, mnist_test_targets.shape)\n",
    "    \n",
    "    return mnist_train_features, mnist_train_targets, mnist_test_features, mnist_test_targets\n",
    "\n",
    "\n",
    "def flatten_features(features):\n",
    "    # Flatten the features from (28, 28) to (784,)\n",
    "    return features.reshape(features.shape[0], -1)\n",
    "\n",
    "def scale_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features)\n",
    "\n",
    "# More general function to load datasets, including solar_flare\n",
    "def load_dataset(dataset_name: str = \"\"):\n",
    "    if dataset_name == \"mnist\":\n",
    "        train_features, train_targets, test_features, test_targets = load_mnist_data()\n",
    "        train_features = flatten_features(train_features)\n",
    "        test_features = flatten_features(test_features)\n",
    "        train_features = scale_features(train_features)\n",
    "        test_features = scale_features(test_features)\n",
    "        \n",
    "    elif dataset_name == \"solar_flare\":\n",
    "        # Load the solar flare dataset\n",
    "        solar_flare = fetch_ucirepo(id=89)\n",
    "        \n",
    "        # Simplifying slightly for the sake of this example\n",
    "        solar_flare.data.targets = solar_flare.data.targets['severe flares']\n",
    "        \n",
    "        # Split the solar flare dataset into train and test sets (90:10 split)\n",
    "        train_features, test_features, train_targets, test_targets = train_test_split(\n",
    "            solar_flare.data.features, solar_flare.data.targets, test_size=0.1, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Onehot encode modified Zurich class, largest spot size, spot distribution\n",
    "        onehot_columns = [\"modified Zurich class\", \"largest spot size\", \"spot distribution\"]\n",
    "        for col in onehot_columns:\n",
    "            onehot = pd.get_dummies(train_features[col], prefix=col)\n",
    "            train_features = pd.concat([train_features, onehot], axis=1)\n",
    "            train_features.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "            onehot = pd.get_dummies(test_features[col], prefix=col)\n",
    "            test_features = pd.concat([test_features, onehot], axis=1)\n",
    "            test_features.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "        # Scale the features\n",
    "        train_features = scale_features(train_features)\n",
    "        test_features = scale_features(test_features)\n",
    "        \n",
    "        # Convert targets to numpy arrays\n",
    "        train_targets = train_targets.to_numpy()\n",
    "        test_targets = test_targets.to_numpy()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "        \n",
    "    # train-test split\n",
    "    train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.2)\n",
    "    \n",
    "    return train_features, train_targets, val_features, val_targets, test_features, test_targets\n",
    "\n",
    "\n",
    "# Train\n",
    "def train_model(model, train_features, train_targets, validation_features, validation_targets, \n",
    "                test_features=None, test_targets=None, learning_rate=0.0015, epochs=80, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train a neural network model on the provided data.\n",
    "    \n",
    "    Parameters:\n",
    "        model: PyTorch model to train\n",
    "        train_features: Training features as numpy array\n",
    "        train_targets: Training targets as numpy array\n",
    "        validation_features: Validation features as numpy array\n",
    "        validation_targets: Validation targets as numpy array\n",
    "        test_features: Test features as numpy array (optional)\n",
    "        test_targets: Test targets as numpy array (optional)\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained model, training loss list, validation accuracy list)\n",
    "    \"\"\"\n",
    "    # Initialize tracking lists\n",
    "    train_loss_list = np.zeros(epochs)\n",
    "    validation_accuracy_list = np.zeros(epochs)\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    train_inputs = torch.from_numpy(train_features).float()\n",
    "    train_targets = torch.from_numpy(train_targets).long()\n",
    "    \n",
    "    validation_inputs = torch.from_numpy(validation_features).float()\n",
    "    validation_targets = torch.from_numpy(validation_targets).long()\n",
    "    \n",
    "    if test_features is not None and test_targets is not None:\n",
    "        test_inputs = torch.from_numpy(test_features).float()\n",
    "        test_targets = torch.from_numpy(test_targets).long()\n",
    "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(train_inputs, train_targets)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataset = TensorDataset(validation_inputs, validation_targets)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        train_inputs = train_inputs.cuda()\n",
    "        validation_inputs = validation_inputs.cuda()\n",
    "        validation_targets = validation_targets.cuda()\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_inputs, batch_targets in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_inputs, batch_targets = batch_inputs.cuda(), batch_targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients to zero\n",
    "            outputs = model(batch_inputs)  # Forward pass with current batch\n",
    "            loss = loss_func(outputs, batch_targets)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            running_loss += loss.item() * batch_inputs.size(0)\n",
    "        \n",
    "        # Store average epoch loss\n",
    "        train_loss_list[epoch] = running_loss / len(train_dataset)\n",
    "        scheduler.step()  # Update learning rate with cosine annealing\n",
    "        \n",
    "        # Compute Validation Accuracy\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for val_inputs, val_targets in validation_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    val_inputs, val_targets = val_inputs.cuda(), val_targets.cuda()\n",
    "                outputs = model(val_inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += val_targets.size(0)\n",
    "                correct += (predicted == val_targets).sum().item()\n",
    "            \n",
    "            validation_accuracy_list[epoch] = correct / total\n",
    "    \n",
    "    # Compute test accuracy if test data is provided\n",
    "    test_accuracy = None\n",
    "    if test_features is not None and test_targets is not None:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    test_inputs, test_targets = test_inputs.cuda(), test_targets.cuda()\n",
    "                outputs = model(test_inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += test_targets.size(0)\n",
    "                correct += (predicted == test_targets).sum().item()\n",
    "            \n",
    "            test_accuracy = correct / total\n",
    "    \n",
    "    return model, train_loss_list, validation_accuracy_list, test_accuracy\n",
    "\n",
    "\n",
    "# Visualize and evaluate\n",
    "def visualize_training(train_loss_list, validation_accuracy_list):\n",
    "    \"\"\"\n",
    "    Visualize training loss and validation accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        train_loss_list: List of training losses\n",
    "        validation_accuracy_list: List of validation accuracies\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12, 6))\n",
    "\n",
    "    # Visualize training loss with respect to iterations (1 iteration -> single batch)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_loss_list, linewidth = 3)\n",
    "    plt.ylabel(\"training loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    sns.despine()\n",
    "\n",
    "    # Visualize validation accuracy with respect to epochs\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(validation_accuracy_list, linewidth = 3, color = 'gold')\n",
    "    plt.ylabel(\"validation accuracy\")\n",
    "    sns.despine()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c39b73",
   "metadata": {},
   "source": [
    "## Define Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e4975",
   "metadata": {},
   "source": [
    "- Here you should make two models, one linear and one convolutional. The agents will use these later. \n",
    "- Copilot/similar tools are encouraged for this; remember that they are agents too!\n",
    "- Also you can add more if you want\n",
    "- Also make sure to reshape the input data to the correct shape. Each model might be given mnist data or later solar_flare data. But these have different shapes! Mnist is AxBx1, but solar_flare is just Ax1\n",
    "    - You don't need to perfect this on your first pass through, just make sure both with mnist and you can revisit later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8355384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcnClassification(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self): # Feel free to add parameters here\n",
    "        super(fcnClassification, self).__init__()\n",
    "        \n",
    "        self.description = \"Trying linear1, linear 2, relu, dropout\" # (short str description of the model)\n",
    "        self.layer1 = torch.nn.Linear(10) # First layer\n",
    "        self.layer2 = torch.nn.Linear(10) # Second layer\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2) # Dropout layer with 20% dropout rate\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "# Uses convolutional layers\n",
    "class ConvClassification(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvClassification, self).__init__()\n",
    "        \n",
    "        self.description = \"conv1, conv1, relu, dropout\" # YOUR CODE HERE (short str description of the model)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(64 * 14 * 14, 128)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "       \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b73ea",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d2898",
   "metadata": {},
   "source": [
    "### DatasetLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f9976",
   "metadata": {},
   "source": [
    "Here is the definition of the agent that handles dataset loading. The logic is pretty simple, so nothing for you to code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0ce5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoaderAgent(AssistantAgent):\n",
    "    \"\"\"\n",
    "    An agent that specializes in loading and preprocessing datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"DatasetLoader\", **kwargs):\n",
    "        self.available_datasets = [\"mnist\"]\n",
    "        system_message = (\n",
    "            \"I am a dataset loading assistant. I can load and preprocess various datasets for machine learning tasks. \"\n",
    "            f\"Currently I support: {', '.join(self.available_datasets)}. \"\n",
    "        )\n",
    "        super().__init__(name=name, system_message=system_message, **kwargs)\n",
    "        \n",
    "        # Store loaded datasets\n",
    "        self._loaded_datasets = {}\n",
    "    \n",
    "    def get_available_datasets(self):\n",
    "        \"\"\"Return a list of available datasets.\"\"\"\n",
    "        return self.available_datasets\n",
    "    \n",
    "    def load_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Load and preprocess a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to load\n",
    "            \n",
    "        Returns:\n",
    "            dict: Information about the loaded dataset and the data itself\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the dataset using the existing function\n",
    "            train_features, train_targets, val_features, val_targets, test_features, test_targets = load_dataset(dataset_name)\n",
    "            \n",
    "            # Store the dataset\n",
    "            self._loaded_datasets[dataset_name] = {\n",
    "                \"train_features\": train_features,\n",
    "                \"train_targets\": train_targets,\n",
    "                \"validation_features\": val_features,\n",
    "                \"validation_targets\": val_targets,\n",
    "                \"test_features\": test_features,\n",
    "                \"test_targets\": test_targets,\n",
    "            }\n",
    "            \n",
    "            # Return information about the loaded dataset\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"train_samples\": train_features.shape[0],\n",
    "                \"validation_samples\": val_features.shape[0],\n",
    "                \"test_samples\": test_features.shape[0],\n",
    "                \"feature_dim\": train_features.shape[1]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Failed to load dataset '{dataset_name}': {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def get_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Retrieve a previously loaded dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            dict: The dataset components or None if not found\n",
    "        \"\"\"\n",
    "        return self._loaded_datasets.get(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea863d2",
   "metadata": {},
   "source": [
    "### InterfaceAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0d527",
   "metadata": {},
   "source": [
    "- Setting up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18ca87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't instantiate LlamaForCausalLM model under dtype=torch.int8 since it is not a floating point dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a simple text-generation pipeline\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/modeling_utils.py:4325\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4320\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m load_gguf_checkpoint(checkpoint_files[\u001b[38;5;241m0\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_to_load\u001b[38;5;241m=\u001b[39mdummy_model)[\n\u001b[1;32m   4321\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4322\u001b[0m         ]\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;66;03m# Find the correct dtype based on current state\u001b[39;00m\n\u001b[0;32m-> 4325\u001b[0m     config, torch_dtype, dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43m_get_torch_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4326\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\n\u001b[1;32m   4327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4329\u001b[0m config\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/modeling_utils.py:1239\u001b[0m, in \u001b[0;36m_get_torch_dtype\u001b[0;34m(cls, torch_dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch_dtype` can be one of: `torch.dtype`, `\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`, a string of a valid `torch.dtype` or a `dict` with valid `torch_dtype` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor each sub-config in composite configs, but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1237\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_default_torch_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m# set fp32 as the default dtype for BC\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     default_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/modeling_utils.py:2163\u001b[0m, in \u001b[0;36mPreTrainedModel._set_default_torch_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;124;03mChange the default dtype and return the previous one. This is needed when wanting to instantiate the model\u001b[39;00m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;124;03munder specific dtype.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[38;5;124;03m`torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.\u001b[39;00m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m-> 2163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model under dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m since it is not a floating point dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2165\u001b[0m     )\n\u001b[1;32m   2167\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstantiating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model under default dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2168\u001b[0m dtype_orig \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n",
      "\u001b[0;31mValueError\u001b[0m: Can't instantiate LlamaForCausalLM model under dtype=torch.int8 since it is not a floating point dtype"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a simple text-generation pipeline\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "# Wrapper function for the local model pipeline\n",
    "def local_model_generate(prompt):\n",
    "    output = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ac639",
   "metadata": {},
   "source": [
    "#### Agent definition\n",
    "\n",
    "- This is the definition of the actual agent you will converse with\n",
    "- Implement the query and get dataset commands\n",
    "- You can come back to the train command after you get to the model trainer agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent definition\n",
    "class InterfaceAgent(ConversableAgent):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        \n",
    "        self.dataset_loader_agent = None\n",
    "        self.model_trainer_agent = None\n",
    "\n",
    "    def set_dataset_loader_agent(self, agent):\n",
    "        self.dataset_loader_agent = agent\n",
    "        \n",
    "    def set_model_trainer_agent(self, agent):\n",
    "        self.model_trainer_agent = agent\n",
    "    \n",
    "    def generate_reply(self, messages):\n",
    "        \"\"\"\n",
    "        There is a lot of obtuse text parsing and such here. I would describe the code as \"technically functional\".\n",
    "        This is becasue the LLM is so limited. With a better LLM, and/or ideally one trained explicitly for \n",
    "        agentic implementation, this would be much cleaner and more flexible. \n",
    "        C.f. retrieval augmented generation, etc.\n",
    "        \"\"\"\n",
    "        # Extract the latest user message\n",
    "        user_message = messages[-1][\"content\"]\n",
    "        \n",
    "        # Check if the message is a \"get dataset\" command\n",
    "        # get dataset <dataset_name>\n",
    "        if \"get dataset\" in user_message:\n",
    "            dataset_name = user_message.replace(\"get dataset\", \"\").strip()\n",
    "            \n",
    "            # Use the DatasetLoaderAgent to load the dataset\n",
    "            result = self.dataset_loader_agent.load_dataset(dataset_name)\n",
    "\n",
    "            # Check if the dataset was successfully loaded\n",
    "            if result[\"status\"] == \"success\":\n",
    "                response_text = (\n",
    "                    f\"Successfully loaded dataset '{result['dataset_name']}'.\\n\"\n",
    "                    f\"Training samples: {result['train_samples']}, \"\n",
    "                    f\"Validation samples: {result['validation_samples']}, \"\n",
    "                    f\"Test samples: {result['test_samples']}, \"\n",
    "                    f\"Feature dimension: {result['feature_dim']}.\"\n",
    "                )\n",
    "            else:\n",
    "                response_text = f\"Failed to load dataset '{dataset_name}': {result['message']}\"\n",
    "        \n",
    "        # Check if the message is a \"query\" command\n",
    "        # query <dataset_name>\n",
    "        elif \"query\" in user_message:\n",
    "            query = user_message.replace(\"query\", \"\").strip()\n",
    "            \n",
    "            # Use the DatasetLoaderAgent to retrieve the dataset\n",
    "            # Assuming the query is in the format \"query <dataset_name>\"\n",
    "            # In principle, this is where you could have an LLM parse the command\n",
    "            dataset_name = query.split()[0]\n",
    "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
    "    \n",
    "            if dataset:\n",
    "                # Prepare a prompt with dataset information\n",
    "                prompt = (\n",
    "                    f\"The dataset '{dataset_name}' has been loaded. \"\n",
    "                    f\"The first two rows of the training features are:\\n\"\n",
    "                    f\"{dataset['train_features'][:2]}.\\n\"\n",
    "                    \"Short, concise description of the dataset:\\n\"\n",
    "                )\n",
    "                # Generate a description using the local model\n",
    "                response_text = local_model_generate(prompt)\n",
    "            else:\n",
    "                response_text = f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
    "                \n",
    "            \n",
    "        elif \"train\" in user_message:\n",
    "            # Should be of the form \"train <dataset_name> <model_type>\"\n",
    "            # Again, this is where a specialized LLM would be really useful\n",
    "            # Model type\n",
    "            parts = user_message.split()\n",
    "            dataset_name = parts[1]\n",
    "            \n",
    "            # Use the ModelTrainerAgent to train the model\n",
    "            if self.model_trainer_agent is not None:\n",
    "                # If the user specified an available model type, use that\n",
    "                result = self.model_trainer_agent.train_model(dataset_name)\n",
    "                \n",
    "                if result[\"status\"] == \"success\":\n",
    "                    response_text = f\"Successfully trained the model.\"\n",
    "                else:\n",
    "                    response_text = f\"Failed to train the model: {result['response']}\"\n",
    "            else:\n",
    "                response_text = \"Model trainer agent is not set.\"\n",
    "        \n",
    "        # No special command, just a normal message. Can just use the model as a chatbot\n",
    "        else:\n",
    "            response_text = local_model_generate(user_message)\n",
    "        \n",
    "        # Return the response in the Autogen format\n",
    "        return {\"role\": \"assistant\", \"content\": response_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e65149",
   "metadata": {},
   "source": [
    "## Talk to the datasetloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d2ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'mnist'.\n",
      "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the agents\n",
    "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
    "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
    "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
    "\n",
    "# Simulate a conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
    "\n",
    "# InterfaceAgent processes the first message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# # Add a second message to the conversation\n",
    "# messages.append({\"role\": \"user\", \"content\": \"query mnist\"})\n",
    "\n",
    "# # InterfaceAgent processes the second message\n",
    "# response = local_agent.generate_reply(messages)\n",
    "# print(response[\"content\"])\n",
    "# print()\n",
    "\n",
    "# # You can also use the model as a chatbot\n",
    "# messages.append({\"role\": \"user\", \"content\": \"Why don't whales have feet?\"})\n",
    "# response = local_agent.generate_reply(messages)\n",
    "# print(response[\"content\"])\n",
    "# print()\n",
    "\n",
    "# Note that the output is not reliable at all since the model is tiny. Sometimes it's surprisingly good though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cf5e8",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78b0a6",
   "metadata": {},
   "source": [
    "This is the agent that actually trains a model for analyzing the dataset. In InterfaceAgent, the train command should generate a description of the dataset and feed it to the train_model_on_query method below (if the user didn't explicitly specify a model type). So use the LLM to generate a recommended model type from the description (\"query\"). \n",
    "\n",
    "If you want, you could implement some more complex logic here, getting the LLM to recommend hyperparameters and things like that. There is also some amount of talking back and forth with itself here, in a chain-of-reasoning style. With a much bigger model, you could start to see some really interesting behavior here. This is what gives DeepSeek its power!\n",
    "\n",
    "#### Note\n",
    "\n",
    "Be careful with how you handle the input dimensions for your models. Convolutional networks want different dimensions than an fully linear network. I recommend making it so that you can feed them the data, and the model itself handles any reshaping needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainerAgent(ConversableAgent):\n",
    "    \"\"\"\n",
    "    An agent that selects, creates, and trains a classification model based on the user's query.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"ModelTrainer\", dataset_loader_agent=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.dataset_loader_agent = dataset_loader_agent\n",
    "        \n",
    "        self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = None, None, None, None\n",
    "        \n",
    "        self.available_models = [\"linear\", \"conv\"]\n",
    "        \n",
    "\n",
    "    def train_model_on_query(self, dataset_name, query, override=None):\n",
    "        \"\"\"\n",
    "        Parse the query, select the model, and train it on the dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            # Retrieve the dataset from the DatasetLoaderAgent\n",
    "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
    "            if not dataset:\n",
    "                return f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
    "            \n",
    "            # Extract dataset components\n",
    "            train_features = dataset[\"train_features\"]\n",
    "            train_targets = dataset[\"train_targets\"]\n",
    "            val_features = dataset[\"validation_features\"]\n",
    "            val_targets = dataset[\"validation_targets\"]\n",
    "            test_features = dataset[\"test_features\"]\n",
    "            test_targets = dataset[\"test_targets\"]\n",
    "            \n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            # Get a response from the LLM recommending a dataset based on query\n",
    "\n",
    "            model_type =  \"\" # parse the response to figure out what model type it recommends\n",
    "\n",
    "            \n",
    "            if len(train_features.shape) == 3:\n",
    "                indim = train_features.shape[1]*train_features.shape[2]\n",
    "            else:\n",
    "                indim = train_features.shape[1]\n",
    "            \n",
    "            # Training printing\n",
    "            print(f\"Training model of type '{model_type}' on dataset '{dataset_name}' with input dimension {indim}.\")\n",
    "            print(f\"Training features shape: {train_features.shape}\")\n",
    "            print(f\"Training targets shape: {train_targets.shape}\")\n",
    "            \n",
    "            # YOUR CODE BELOW (arguments to the models)\n",
    "            \n",
    "            # Select the model based on the model_type\n",
    "            if model_type == \"linear\" or override == \"linear\":\n",
    "                model = fcnClassification()\n",
    "            elif model_type == \"conv\" or override == \"conv\":\n",
    "                model = ConvClassification()\n",
    "            else: # Fallback\n",
    "                model = ConvClassification()\n",
    "\n",
    "            # Train the model\n",
    "            self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = train_model(\n",
    "                model, train_features, train_targets, val_features, val_targets,\n",
    "                test_features=test_features, test_targets=test_targets\n",
    "            )\n",
    "\n",
    "            # Summarize the training results\n",
    "            response = (\n",
    "                f\"Model '{model_type}' trained successfully on dataset '{dataset_name}'.\\n\"\n",
    "                f\"Final validation accuracy: {self.val_accuracy_list[-1]:.4f}\\n\"\n",
    "                f\"Test accuracy: {self.test_accuracy:.4f}\"\n",
    "            )\n",
    "            print(\"Training response:\", response)\n",
    "            \n",
    "            result = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"status\": \"success\",\n",
    "                \"model\": self.model,\n",
    "                \"train_loss_list\": self.train_loss_list,\n",
    "                \"val_accuracy_list\": self.val_accuracy_list,\n",
    "                \"test_accuracy\": self.test_accuracy,\n",
    "                \"response\": response\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"response\": f\"An error occurred during training: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ef0c7",
   "metadata": {},
   "source": [
    "## Train a model on mnist\n",
    "\n",
    "- Get the dataset and train a model on it\n",
    "- You can try specifying, but make sure the LLM can recommend a model type itself\n",
    "- The mnist dataset is very well known, so it should decide on the convolutional model itself (up to the output being bad becasue of the small model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'mnist'.\n",
      "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 3.92 MB, max allowed: 9.07 GB). Tried to allocate 584.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery mnist\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# InterfaceAgent processes the second message\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[40], line 65\u001b[0m, in \u001b[0;36mInterfaceAgent.generate_reply\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     58\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been loaded. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first two rows of the training features are:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_features\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShort, concise description of the dataset:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Generate a description using the local model\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_model_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not loaded or does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[56], line 19\u001b[0m, in \u001b[0;36mlocal_model_generate\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlocal_model_generate\u001b[39m(prompt):\n\u001b[0;32m---> 19\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mllm_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1379\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         )\n\u001b[1;32m   1377\u001b[0m     )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1386\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1385\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1386\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/generation/utils.py:3431\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3428\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3431\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:315\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    312\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    313\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 315\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    319\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    320\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/newenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:81\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     79\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     80\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 81\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 3.92 MB, max allowed: 9.07 GB). Tried to allocate 584.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Create instances of the agents\n",
    "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
    "model_trainer_agent = ModelTrainerAgent(name=\"ModelTrainer\", dataset_loader_agent=dataset_loader_agent)\n",
    "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
    "\n",
    "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
    "local_agent.set_model_trainer_agent(model_trainer_agent)\n",
    "\n",
    "# Simulate a conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
    "# InterfaceAgent processes the first message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "# Add a second message to the conversation\n",
    "messages.append({\"role\": \"user\", \"content\": \"query mnist\"})\n",
    "# InterfaceAgent processes the second message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# And you now have a trained model!\n",
    "print(model_trainer_agent.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9304e7a",
   "metadata": {},
   "source": [
    "## Repeat with solar_flare\n",
    "\n",
    "Now that everything's working, try adding the solar_flare dataset in.\n",
    "\n",
    "- Add solar_flare to the list of available datasets in the datasetloader\n",
    "    - The backend stuff for this is already in the lab 3 code\n",
    "        - This would not be too bad to fit into the agents in principle\n",
    "- The other agents shouldn't need any additional modification, unless you wrote mnist-specific code\n",
    "    - Ignore that the models have mnist in the name, they aren't necessarily mnist specific\n",
    "    - This is easy to do without intending to. The more generalized your code, the better\n",
    "    - Try throwing any errors you get to copilot as a first pass. It's good at generalizing code\n",
    "\n",
    "- In the end, you should be able to tell the agent \"train 'dataset' 'random'\", and it should be able to select and train a linear model if dataset=solar_flare or a convolutional model if dataset=mnist\n",
    "    - Of course, up to LLM inconsistency\n",
    "    - With a good model/more complex logic, it should be able to do this for many different models and any random dataset you throw at it\n",
    "        - Very useful for taking a first crack at a dataset and getting a starting point\n",
    "        - This entire step could also be implemented as a \"copilot, figure out how to add 'dataset'\" command with a model as good as copilot hooked into this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f02416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (just more queries to the agents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
